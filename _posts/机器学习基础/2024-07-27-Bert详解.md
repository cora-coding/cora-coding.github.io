---
layout: post
title: "Bert详解"
date: 2024-06-10
author: Cola Liu
categories: [机器学习基础]
---

**BERT**（**Bidirectional Encoder Representations from Transformers**） 是一个语言表示模型。

[Bidirectional] 双向
传统的语言模型就是预测下一个词，例如我们键盘的输入法。一般采用的是从左到右的顺序。但是这就限制了模型的能力，不能考虑到后面的序列的信息。如果要考虑双向的信息的话，可以再使用从右到左的顺序，预测下一个词，然后再将两个模型融合，这样子就考虑到了双向的信息（这个是ELMO的做法）

参数量直接double

[简介]
它的主要模型结构是**trasnformer**的**encoder**堆叠而成。训练过程分为 **pretraining** 与 **finetuning**（微调）两个阶段。


[CLS]
在 BERT 模型中，[CLS] 是一个特殊的标记
[SEP]
分隔符

输入层，中间层，以及输出层 与 encoder层、decoder层有什么关系？

[Position Embedding]
BERT使用的是不是transformer对应的函数型(functional)的encoding方式，而是直接采用类似word embedding的方式（Parametric），直接获得position embedding。

- word embedding 基于训练
- functional embedding 基于函数


[输入层](position embedding + segment embedding + token embedding)
输入层为三个embedding相加（position embedding + segment embedding + token embedding）

    [segment embedding]
    用[CLS]和[SEP]区分两个不同的句子

    [token embedding]
    词嵌入的生成可以基于 BERT 预训练的词汇表（vocabulary）

[中间层]
同 Transformer 中的 encoder，都是由self-attention layer + ADD&BatchNorm layer + FFN 层组成的

12层encoder  hidden_dim=768, 参数量 110m
24层encoder hidden_dim=1024, 参数量 340m

[输出层]
输出的是概率分布，要自己设计吗

- 句子分类任务（Sentence Classification）
经过一个全连接层（分类器），输出类别的概率分布。
- QA：段落中的某个子串作为问题的答案。
打分高的作为开始位置和结束位置，也是概率分布
- NSP Next Sentence Prediction 两个**句子**是否是连续句子
- 标注任务 如 NER 命名实体识别
- 序列生成任务，如文本生成

[应用层面的代码]
load model + 流程