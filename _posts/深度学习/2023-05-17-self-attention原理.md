---
layout: post
title: "self-attention原理"
date: 2023-05-17
author: Cola Liu
categories: [深度学习]
---

`self-attention`是一个新的模型架构。对待新的模型架构，一般是从已有的知识入手，分析新旧模型之间的异同，提出新的架构可以帮助以前的模型解决什么痛点。

## self-attention的提出

### 从简单的模型架构开始

我们先来看常规的模型架构，通常，我们只有一个输入输出（偶尔会有多个输出，比如给出一张动物的图片，判断这张图片中的小动物是猫还是狗狗，但是本质上还是一个输出，输出的是这张图片是猫或狗的概率。）

<img src="/assets/imgs/ai/self-attention/单输入输出.png" width="300" />

输入的`x`经常是一个`vector`或者一整个表格，然后我们把它拼成一个长`vector`。

### 处理多输入

那么问题来了，如果我们有多个输入呢？也就是输入有多个`vector`呢？如下图所示，我们有4个输入，然后对应有4个输出 ⬇️

<img src="/assets/imgs/ai/self-attention/多输入输出-1.png" width="500" />

很简单，我们把这4个输入拆开成单个，逐一输入计算就完事～或者干脆作为一个矩阵丢进去计算也行！这个问题暂且就解决了。

<img src="/assets/imgs/ai/self-attention/多输入输出-2.png" width="500" />

### 输入数据之间有联系

等等，我们再考虑其他的情况：

如果这4个输入彼此之间是有一定关系的呢？比如，`i saw a saw`，此`saw`非彼`saw`。

特别是在语言分析中，字与字之间的关系不同，含义就不同。"小心地滑"里面的地，可以解释为“地面”或者“得的地”中的“地”，后加动词。

<img src="/assets/imgs/ai/self-attention/多输入输出-3.png" width="500" />

显然，这里考虑了输入与输入之间的“先后”关系，可为“位置关系”，也可以理解为“时间关系”。

这里有一个常规的解决方法是，把前面`x1`的计算结果`a1`,作为下一个`x2`的输入。这样一来，“先后”关系得以保存下来。

<img src="/assets/imgs/ai/self-attention/rnn架构.png" width="500" />

这就是我们常常听到的RNN架构，原理非常简单，即把前面的输出结果作为后面计算过程的输入。这是一种常规的串行的解决方法。

但是由于串行效率过低，并且越前面的信息由于计算次数越多，会导致信息的逐级递减。因此，有人提出了`self-attention`这个模型架构。

<img src="/assets/imgs/ai/self-attention/self-attention-1.png" width="500" />

`self-attention`能够很好地解决RNN中串行计算的问题，它的原理是计算向量与向量之间的位置关系/相似度。

### self-attention 具体架构

我们来看一下`self-attention`的架构。

- 1、将输入`vector` `x1` 与一个权重矩阵 `Wq` 相乘，得到 `q1`；
- 2、将输入`vector` `x1` 与另一个权重矩阵 `Wk` 相乘，得到 `k1`；
- 3、以此类推得到`q2`、`k2`、`q2`、`k3`等；

<img src="/assets/imgs/ai/self-attention/self-attention-2.png" width="400" />

> 到此为止，我们的参数只有 **Wq** 和 **Wk**

- 4、对于每一个`q1`，我们用`k1`、`k2`、`k3`、`k4`与之做点乘（dot product）

> 这里就可以体现出向量与向量之间的关系了。何以见得呢？

#### 向量点乘的几何意义

我们知道，向量点乘的结果是一个标量，即一个数。先来看看向量点乘是怎么定义的：

<img src="/assets/imgs/ai/self-attention/向量点乘的计算过程.png" width="500"/>

用数学式子表示还是比较抽象，回到向量点乘的几何意义。可以看到，向量点乘的几何意义主要有以下几个点：

- **投影**：向量的点乘可以用来计算一个向量在另一个向量方向上的投影长度。

- **夹角**：向量的点乘还可以用来计算两个向量之间的夹角。
    这个夹角θ表示了两个向量之间的方向一致程度，当两个向量平行时夹角为0°，当两个向量垂直时夹角为90°。

<img src="/assets/imgs/ai/self-attention/向量点乘的几何意义.png" width="300"/>

以上简单介绍了点乘这个操作如何体现输入vector之间的位置关系以及相似度的。接着我们继续看self-attention的计算过程。

- 5、将输入`vector` `x1` 与一个权重矩阵 `Wv` 相乘，得到 `v1`；以此类推

> 到此为止，我们的参数只有 **Wq** 、**Wk** 和 **Wv**

- 6、将步骤4得到的点乘结果`α11`（q1·k1）乘于`v1`，`α12`（q1·k2）乘于`v2`,...
- 7、将上述结果 **累加** 可以得到`y1`

<img src="/assets/imgs/ai/self-attention/self-attention-3.png" />

- 8、关于`y2`、`y3`、`y4`的计算过程跟`y1`的计算过程相同。

## 小结

可以看出，`self-attention`架构的设计，主要利用了向量（矩阵）的几何意义来保存相关的位置关系的。这是一个很好的解决问题的思路之一。

`self-attention`与 传统的 `RNN` 相比，从串行计算变成并行计算，可以大大提升整个模型的运行效率。

两者设计思路有着明显的不同。

> `RNN` 从传统的输入输出出发，考虑的是输入输出的时间前后问题，是一个比较常规的解决问题的思路。

> 而 `self-attention` 直接从输入本身入手，观察数据与数据之间天然的几何意义，计算它们之间的位置关系与距离，脱离了传统的时间序列等待的模式，这种思路非常值得学习。

具体为什么设计出q、k、v这三个参数，个人暂时还不大了解，以后学到了再来解答。
